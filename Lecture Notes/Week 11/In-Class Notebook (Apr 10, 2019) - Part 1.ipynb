{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSDS 631 - Lecture 11 (April 10, 2019)\n",
    "\n",
    "## End-to-End Analysis\n",
    "\n",
    "Now that we're comfortable with the basic functionality of Python and Pandas, I want to go through an example of what you can do with all of the tools we've discussed (plus a few extra that we haven't covered yet).\n",
    "\n",
    "We have three data files that we're mostly going to read from:\n",
    "- Users: Describes users overall characteristics\n",
    "- Reviews: Details about individual reviews\n",
    "- Businesses: Details about the businesses our reviewers rated\n",
    "\n",
    "The reviews in these files are primarily focused on reviews in Las Vegas, Toronto, and Phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### These are my file path fetchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to help me get all of the files in a folder so that I can open them\n",
    "def get_file_paths(folder, full_path=True):\n",
    "    file_paths = []\n",
    "    for (dirpath, dirnames, filenames) in os.walk(folder):\n",
    "        for file_ in filenames:\n",
    "            if (not re.match('\\.', file_)) and (file_ != '_SUCCESS'):\n",
    "                if full_path:\n",
    "                    file_paths.append(os.path.join(dirpath, file_))\n",
    "                else:\n",
    "                    file_paths.append((dirpath, file_))\n",
    "    return file_paths\n",
    "\n",
    "\n",
    "def get_all_paths(subfolder):\n",
    "    folder = os.path.join('/Users/jasonshu/Documents/yelp', subfolder)\n",
    "    files = get_file_paths(folder)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = {}\n",
    "file_paths['reviews'] = get_all_paths('reviews')\n",
    "file_paths['users'] = get_all_paths('users')\n",
    "file_paths['businesses'] = ['/Users/jasonshu/Documents/yelp/yelp_businesses.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in file_paths:\n",
    "    print(key, len(file_paths[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### These are my file openers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_single_review_file(filepath):\n",
    "    data = []\n",
    "    with open(filepath,  'r') as f:\n",
    "        reviews = f.read().strip().split('\\n')\n",
    "    for review in reviews:\n",
    "        temp = json.loads(review)\n",
    "        del(temp['text'])\n",
    "        data.append(temp)\n",
    "    return data\n",
    "\n",
    "\n",
    "def open_single_user_file(filepath):\n",
    "    data = []\n",
    "    elite = []\n",
    "    with open(filepath,  'r') as f:\n",
    "        users = f.read().strip().split('\\n')\n",
    "    for user in users:\n",
    "        temp = json.loads(user)\n",
    "        data.append(temp)\n",
    "    return data\n",
    "\n",
    "\n",
    "def open_business_data(filepath):\n",
    "    data = []\n",
    "    with open(filepath,  'r') as f:\n",
    "        businesses = f.read().strip().split('\\n')\n",
    "    for business in businesses:\n",
    "        temp = json.loads(business)\n",
    "        data.append(temp)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview = {}\n",
    "preview['reviews'] = open_single_review_file(file_paths['reviews'][0])\n",
    "preview['users'] = open_single_user_file(file_paths['users'][0])\n",
    "preview['businesses'] = open_business_data(file_paths['businesses'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing you ALWAYS do when opening data for the first time is to look at the data type that you have loaded. JSON parsers can return lists or dictionaries, and this will be what allows you to know what you are dealing with and how to access the data.\n",
    "\n",
    "Once we've done that, we look at a few values to figure out what we have that could be of use for an analysis. I'm doing that for each of our data sources.\n",
    "\n",
    "##### Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(preview['reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preview['reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview['reviews'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(preview['users'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preview['users'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview['users'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(preview['businesses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preview['businesses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview['businesses'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Estimated number of observations per type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in file_paths:\n",
    "    num_files = len(file_paths[key])\n",
    "    obs_per_file = len(preview[key])\n",
    "    print(key, num_files * obs_per_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I know what the data looks like, I'm going to write some functions to get exactly the data I want out of each dictionary from each data source. I don't want to carry around all of the data because it's overwhelming and unnecessary. This is where you need a specific problem in mind to solve so that you can guess ahead of time what you think will be necessary and what you think will never be used.\n",
    "\n",
    "Sometimes you will want to loop through each item and get only the data you need, then create your DataFrames. Other times (when you are lucky) you can create the DataFrame directly from the data, then get rid of what you don't want. In this case, I have to parse the user data, but I can load the review data directly from the list of dictionaries I have opened. The business data is a little wonky because there is a dictionary as one of the values, which won't really work. However, since the data is not going to be kept, I will load it directly as a DataFrame and then drop it.\n",
    "\n",
    "NOTE: Loading data as a DataFrame and dropping unwanted fields is much faster than using for-loops and parsing along the way. However, it is much more resource intensive for your computer to hold all of the data in a DataFrame, so your computer may not actually be able to do it every time. This is a trial and error thing until you do it enough times to know what you can and cannot load.\n",
    "\n",
    "#### These are my file parsers for users and businesses\n",
    "\n",
    "For users, I want some raw data, but I also want to \"create\" new data from the data I am seeing. For instance, I don't care about who the individual user's friends are - that's not helpful in this context. I do, however, think it could be useful to know *how many* friends they have. I also want to know the years they were \"Elite\" because I think that may be interesting down the road. Since that data comes to us in a different form and does not align 1-to-1, there is no easy way to store that data as an attribute of the user. Thus, I have to create an entirely different way of storing the users' elite years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_single_user_dict(user):\n",
    "    base_fields = ['user_id', 'name', 'review_count', 'average_stars', 'yelping_since', 'fans', 'cool', 'funny', 'useful']\n",
    "    base_data = [user[field] for field in base_fields]\n",
    "    num_friends = len(user['friends'].split(', '))\n",
    "    base_data.append(num_friends)\n",
    "    if user['elite']:\n",
    "        years_elite = user['elite'].split(',')\n",
    "        user_id = user['user_id']\n",
    "        elite_list = []\n",
    "        for year in years_elite:\n",
    "            elite_list.append([user_id, year])\n",
    "    else:\n",
    "        elite_list = []\n",
    "    return base_data, elite_list\n",
    "\n",
    "def parse_single_business_dict(business):\n",
    "    base_fields = ['business_id', 'name', 'review_count', 'stars', 'city', 'state', 'postal_code']\n",
    "    base_data = [business[field] for field in base_fields]\n",
    "    return base_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_single_user_dict(preview['users'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have my parser and file loading strategy, I will write a function that allows me to convert each of the files I open into a DataFrame. This is ultimately where we want 90% of our analyses to wind up (in a DataFrame). I will combine the knowledge I have of the structure of the file with the user parser I just wrote to do this.\n",
    "\n",
    "##### File openers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reviews_from_file_as_df(reviews_list_of_dicts):\n",
    "    kept_columns = ['review_id', 'user_id', 'business_id', 'stars', 'date']\n",
    "    reviews_df = pd.DataFrame(reviews_list_of_dicts)\n",
    "    subset_df = reviews_df[kept_columns].copy()\n",
    "    return subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_users_from_file_as_df(users_list_of_dicts):\n",
    "    user_data = []\n",
    "    elite_data = []\n",
    "    for user_dict in users_list_of_dicts:\n",
    "        user, elite = parse_single_user_dict(user_dict)\n",
    "        user_data.append(user)\n",
    "        elite_data += elite\n",
    "    user_df = pd.DataFrame(user_data)\n",
    "    elite_df = pd.DataFrame(elite_data)\n",
    "    user_df.columns = ['user_id', 'name', 'review_count', 'average_stars', 'yelping_since', 'fans', 'cool', \n",
    "                       'funny', 'useful', 'num_friends']\n",
    "    elite_df = ['user_id', 'year']\n",
    "    return user_df, elite_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_businesses_from_file_as_df(businesses_list_of_dicts):\n",
    "    kept_columns = ['business_id', 'name', 'review_count', 'stars', 'city', 'state', 'postal_code']\n",
    "    businesses_df = pd.DataFrame(businesses_list_of_dicts)\n",
    "    subset_df = businesses_df[kept_columns].copy()\n",
    "    return subset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALMOST THERE!\n",
    "\n",
    "Now that we can open a single file, for the data types that are being read from multiple files, I need to open each file, load it as a DataFrame, put each DataFrame into a list, then concatenate them into a single DataFrame.\n",
    "\n",
    "For businesses, even though I don't need to create another function, I am so that the naming convention is consistent and I can keep track of things better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_reviews(list_of_filepaths):\n",
    "    all_reviews = []\n",
    "    num_files = len(list_of_filepaths)\n",
    "    print('Loading reviews')\n",
    "    for n,f in enumerate(list_of_filepaths):\n",
    "        print(f'\\t{n} of {num_files}')\n",
    "        reviews_list_of_dicts = open_single_review_file(f)\n",
    "        reviews_df = load_reviews_from_file_as_df(reviews_list_of_dicts)\n",
    "        all_reviews.append(reviews_df)\n",
    "    all_reviews_df = pd.concat(all_reviews)\n",
    "    return all_reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_users(list_of_filepaths):\n",
    "    all_users = []\n",
    "    all_elite = []\n",
    "    num_files = len(list_of_filepaths)\n",
    "    print('Loading users')\n",
    "    for n, f in enumerate(list_of_filepaths):\n",
    "        print(f'\\t{n} of {num_files}')\n",
    "        users_list_of_dicts = open_single_user_file(f)\n",
    "        users_df, elite_df = load_users_from_file_as_df(users_list_of_dicts)\n",
    "        all_users.append(users_df)\n",
    "        all_elite.append(elite_df)\n",
    "    all_users_df = pd.concat(all_users)\n",
    "    all_elite_df = pd.concat(all_elite)\n",
    "    return all_users_df, all_elite_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_businesses(filepath):\n",
    "    print('Loading businesses')\n",
    "    businesses_list_of_dicts = open_business_data(filepath)\n",
    "    all_businesses_df = load_businesses_from_file_as_df(businesses_list_of_dicts)\n",
    "    return all_businesses_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = dt.now()\n",
    "reviews = load_all_reviews(file_paths['reviews'])\n",
    "t2 = dt.now()\n",
    "print('\\t', t2-t1)\n",
    "users, elite = load_all_users(file_paths['users'])\n",
    "t3 = dt.now()\n",
    "print('\\t', t3-t2)\n",
    "businesses = load_all_businesses(file_paths['businesses'][0])\n",
    "t4 = dt.now()\n",
    "print('\\t', t4-t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Success!!!\n",
    "\n",
    "Loading and cleaning data is the worst part of every analysis. It took me 6 hours just to write the code to just **open** the files, let alone parse it, and transform it into DataFrames. Don't think for a second that any of this comes easy and that you are doing something wrong if you are spinning your wheels trying to get the data to be analyzed. Some figures estimate that 80% of a data scientist's time is spent just loading and cleaning data. The analysis is the **easy** part! This is why we spent so much of our class looking at how to do things in Python. Now that we have the data loaded, it's time to have fun!!\n",
    "\n",
    "## Analyses\n",
    "\n",
    "In this section, I will perform three analyses that answer different questions about Yelp users and businesses, including:\n",
    "1. Can I make up of \"personas\" for Yelp reviewers and find them?\n",
    "2. Do \"Elite\" reviewers tend to take their responsibility more seriously and judge businesses more critically?\n",
    "\n",
    "### Analysis 1a - Finding personas of yelp users\n",
    "\n",
    "Yelp data is one of my favorite data sets because there are so many things you can glean about people. One of the things that intrigues me the most is regarding the different types of \"personalities\" that exist amongst reviewers. A few include:\n",
    "- Hot-Cold Harry: Tends to either love a place or hate it (mostly 5-star and 1-star reviews\n",
    "- Normal Norman: Tends to fit the distribution of reviews in a bell-shaped curve\n",
    "- Negative Nancy: Tends to be mostly a complainer with more 1- and 2-star reviews\n",
    "- Positive Patricia: Tends to give mostly 4- and 5-star reviews. She is either easily satisfied or only goes places that she knows she will like (reinforcement bias)\n",
    "- Content Cassandra: Tends to give mostly positive reviews, centered mostly around 4-star with some 5-star and some 3-star as well\n",
    "\n",
    "So that you might know what these distributions look like, let's just set those up here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distributions = {}\n",
    "distributions['Hot-Cold Harry'] = pd.Series([.3, .1, .05, .15, .4])\n",
    "distributions['Normal Norman'] = pd.Series([.05, .15, .4, .25, .15])\n",
    "distributions['Negative Nancy'] = pd.Series([.3, .4, .1, .1, .1])\n",
    "distributions['Positive Patricia'] = pd.Series([.05, .05, .15, .25, .5])\n",
    "distributions['Content Cassandra'] = pd.Series([0, 0, .2, .5, .3])\n",
    "distributions['Even Evan'] = pd.Series([.2, .2, .2, .2, .2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in distributions:\n",
    "    distributions[name].plot(kind='bar')\n",
    "    print(name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use some techniques used for measuring \"closeness\" or \"similarity\" to assign a reviewer a persona."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get each reviewers score distribution, we need to aggregate their review data. Let's refresh our memories about what the review DataFrame looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to convert the star ratings to integers to make things a little easier for me. Then I'm going to group by the user IDs and star ratings to get the users' review count by rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['stars'] = reviews['stars'].astype(int)\n",
    "reviewer_stars = reviews.groupby(['user_id', 'stars'])[['review_id']].count()\n",
    "reviewer_stars = reviewer_stars.reset_index()\n",
    "reviewer_stars.columns = ['user_id', 'stars', 'review_count']\n",
    "reviewer_stars.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want the data so that it's more easily digestable. We're going to use the handy dandy pivot table to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewer_stars_table = reviewer_stars.pivot_table(index='user_id', columns='stars', values='review_count')\n",
    "reviewer_stars_table.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that several of our users only have a few reviews. Let's only focus on ones that have at least 10 reviews (this is an arbitrary cutoff)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewer_stars_table['total_reviews'] = reviewer_stars_table.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewer_stars_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewer_stars_table_10 = reviewer_stars_table[reviewer_stars_table['total_reviews'] >= 10]\n",
    "reviewer_stars_table_10.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of curiousity, how many reviewers did we have, and how many do we have left after filtering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reviewer_stars_table.shape[0])\n",
    "print(reviewer_stars_table_10.shape[0])\n",
    "print(reviewer_stars_table_10.shape[0]/reviewer_stars_table.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We lost 92.5% of our reviewers!!! Let's see what percentage of reviews were actually written by those who met our threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_analysis = reviewer_stars_table.copy()\n",
    "count_analysis = count_analysis.sort_values('total_reviews', ascending=False)\n",
    "total_reviews = count_analysis['total_reviews'].sum()\n",
    "count_analysis['cum_reviews'] = count_analysis['total_reviews'].cumsum()\n",
    "count_analysis['cum_pct'] = count_analysis['cum_reviews'] / total_reviews\n",
    "count_analysis.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_analysis[count_analysis['total_reviews'] >= 10].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this, it looks like our 10+ count reviewers make up 52.1% of the total reviews in our data set. I can live with that. ok, now let's look at a few random users' star distributions and see what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = reviewer_stars_table_10.sample(5)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in sample.index:\n",
    "    row = sample.loc[user, [1,2,3,4,5]]\n",
    "    row.plot(kind='bar')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now let's see if we can find a way to automatically find which of the five personas that our users are closest to. We're going to use the idea of \"Cosine Similarity.\"\n",
    "\n",
    "<img src=\"https://www.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/2b4a7a82-ad4c-4b2a-b808-e423a334de6f.png\" width=\"350\" height=\"350\"/>\n",
    "\n",
    "The formula for this is:\n",
    "<img src=\"https://neo4j.com/docs/graph-algorithms/current/images/cosine-similarity.png\" width=\"400\" height=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if I had two identical vectors listed below, their similarity should be perfect. Let's see the math work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.Series([1, 2, 1, 4, 2])\n",
    "b = pd.Series([1, 2, 1, 4, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.dot(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_magnitude = np.sqrt((a ** 2).sum())\n",
    "b_magnitude = np.sqrt((b ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a.dot(b)) / (a_magnitude * b_magnitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet!\n",
    "\n",
    "Let's change b a tiny bit and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = pd.Series([1, 2, 1, 3, 2])\n",
    "b_magnitude = np.sqrt((b ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a.dot(b)) / (a_magnitude * b_magnitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shouldn't be interpreted as \"99% similar\", but if that helps you internalize similarity, then go ahead and think that way.\n",
    "\n",
    "Let's go ahead and write the function so that we can compute the similarity of any two vectors (assuming they are the same length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(vec1, vec2):\n",
    "    if isinstance(vec1, list):\n",
    "        vec1 = pd.Series(vec1)\n",
    "    if isinstance(vec2, list):\n",
    "        vec2 = pd.Series(vec2)\n",
    "    dot = vec1.dot(vec2)\n",
    "    magnitude1 = np.sqrt((vec1 ** 2).sum())\n",
    "    magnitude2 = np.sqrt((vec2 ** 2).sum())\n",
    "    similarity = dot / (magnitude1 * magnitude2)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's test our function\n",
    "compute_cosine_similarity(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help ourselves out, we're going to do a couple of things now:\n",
    "1. Create a DataFrame of values for our personas\n",
    "2. Drop the total_reviews column from our table\n",
    "3. Write a function that computes the similarity of a user to each of our personas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personas_df = pd.DataFrame.from_dict(distributions, orient='index')\n",
    "personas_df = personas_df.sort_index()\n",
    "personas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looks like we need to modify the column names\n",
    "personas_df.columns = [1,2,3,4,5]\n",
    "personas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewer_stars_table_10 = reviewer_stars_table_10.drop('total_reviews', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewer_stars_table_10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to fill in the null values with 0 so that we can do our math\n",
    "reviewer_stars_table_10 = reviewer_stars_table_10.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's just see an example of a row\n",
    "one_reviewer_row = reviewer_stars_table_10.loc['---1lKK3aKOuomHnwAkAow']\n",
    "one_reviewer_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_reviewer_distribution = one_reviewer_row / one_reviewer_row.sum()\n",
    "one_reviewer_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_reviewer_distribution.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_against_personas(one_reviewer_distribution, distributions):\n",
    "    axes = []\n",
    "    fig = plt.figure(figsize=(10,8))  \n",
    "    order = sorted(list(distributions.keys()))\n",
    "    for i, persona in enumerate(order):\n",
    "        axes.append(plt.subplot2grid((6,6),(i,5)))\n",
    "    axes.append(plt.subplot2grid((6,6),(0,0),rowspan=6,colspan=5))\n",
    "    for i, persona in enumerate(order):\n",
    "        distributions[persona].plot(kind='bar', ax=axes[i])\n",
    "        axes[i].xaxis.set_visible(False)\n",
    "        axes[i].yaxis.set_visible(False)\n",
    "        axes[i].set_title(persona, fontdict={'fontsize': 8})\n",
    "        axes[i].set_ylim((0,.6))\n",
    "    one_reviewer_distribution.plot(kind='bar', ax=axes[6])\n",
    "    plt.show()\n",
    "plot_against_personas(one_reviewer_distribution, distributions)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_persona_similarity(user_id, user_review_count, personas_df):\n",
    "    all_scores = [user_id]\n",
    "    user_relative_pcts = user_review_count / user_review_count.sum()\n",
    "    order = sorted(distributions.keys())\n",
    "    for persona in order:\n",
    "        persona_distribution = personas_df.loc[persona]\n",
    "        similarity = compute_cosine_similarity(user_review_count, persona_distribution)\n",
    "        all_scores.append(similarity)\n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure_persona_similarity('---1lKK3aKOuomHnwAkAow', one_reviewer_distribution, personas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewer_stars_table_10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_rows(df):\n",
    "    total = df.sum(axis=1)\n",
    "    df_T = df.T\n",
    "    total_T = total.T\n",
    "    scaled_df_T = df_T / total_T\n",
    "    scaled_df = scaled_df_T.T\n",
    "    return scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewer_stars_table_10_scaled = scale_rows(reviewer_stars_table_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewer_stars_table_10_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_similarities = []\n",
    "t1 = dt.now()\n",
    "for i, user_id in enumerate(reviewer_stars_table_10.head(10000).index):\n",
    "    if i % 1000 == 0:\n",
    "        print('{:.1%}'.format(i / reviewer_stars_table_10.shape[0]))\n",
    "    row = reviewer_stars_table_10.loc[user_id]\n",
    "    similarities = measure_persona_similarity(user_id, row, personas_df)\n",
    "    all_similarities.append(similarities)\n",
    "t2 = dt.now()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = reviewer_stars_table_10_scaled.head(10000).index.tolist()\n",
    "columns = ['user_id'] + personas_df.index.tolist()\n",
    "similarities_df = pd.DataFrame(all_similarities, columns=columns, index=ids)\n",
    "similarities_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarities_linalg(reviewer_relative_stars_df, personas_df):\n",
    "    reviewer_matrix = reviewer_relative_stars_df.values.T\n",
    "    personas_matrix = personas_df.values\n",
    "    dot = pd.DataFrame(personas_matrix.dot(reviewer_matrix).T)\n",
    "    user_magnitude = pd.DataFrame(np.sqrt((reviewer_relative_stars_df ** 2).sum(axis=1)))\n",
    "    persona_magnitude = pd.DataFrame(np.sqrt((personas_df ** 2).sum(axis=1)))\n",
    "    magnitude_product = user_magnitude.values.dot(persona_magnitude.values.T)\n",
    "    similarity_scores = dot / magnitude_product\n",
    "    similarity_scores.columns = personas_df.index.tolist()\n",
    "    similarity_scores.set_index(reviewer_relative_stars_df.index, inplace=True)\n",
    "    return similarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_persona_scores_df = compute_similarities_linalg(reviewer_stars_table_10_scaled, personas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_persona_scores_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_personas = user_persona_scores_df.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_personas.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_personas_df = pd.DataFrame(user_personas, columns=['persona'])\n",
    "user_personas_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_user(persona_type, user_personas, reviewer_relative_distributions_table, persona_distributions):\n",
    "    id_ = user_personas[user_personas==persona_type].sample().index[0]\n",
    "    row = reviewer_stars_table_10_scaled.loc[id_]\n",
    "    plot_against_personas(row, distributions)\n",
    "    print(user_persona_scores_df.loc[id_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_random_user('Negative Nancy', user_personas, reviewer_stars_table_10_scaled, distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terrific!\n",
    "\n",
    "One last thing! Let's plot a few different plot types showing the distribution of personas.\n",
    "\n",
    "We want to do the following things:\n",
    "- Plot a bar chart (in descending order) of the distribution of personas\n",
    "- Plot a pie chart (in the same chart as the bar chart) of the distribution of personas\n",
    "- Move the legend for the pie chart\n",
    "- Give each subplot a title\n",
    "- Give the overall graphic a title\n",
    "- Plot small versions of each persona on the right side of the chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persona_sizes = user_personas_df.groupby('persona').size().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persona_sizes.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis 2 - Are Elite reviewers more critical than non-elite users?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elite.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
