{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Statistics and Linear Modeling\n",
    "\n",
    "As a Data Science practitioner rather than theorist, my intent of teaching you statistics is more about proper application and interpretation of methods rather than the theory of how it works. My father is a Statistics PhD. I've personally taken a half-dozen stats classes and gotten an A or A+ in all of them. Yet during all of the learning and all of the conversations about statistics with my dad, nothing seemed to make sense until I started using it for real (and realizing what people do incorrectly based on misunderstandings about the techniques). Thus, we will go over many of the pitfalls that people make rather than build up any sort of foundational knowledge that I'm sure you've already gotten.\n",
    "\n",
    "Over the next two lectures, we will cover the following topics:\n",
    "- Overview of statistics\n",
    "- Role of sampling\n",
    "- Distributions\n",
    "- Basic sample statistics and inference\n",
    "- Hypothesis testing and errors\n",
    "- Use cases and interpretation of linear modeling\n",
    "- Bias-variance tradeoff of linear modeling\n",
    "- Problem structuring and responsible inferences\n",
    "- Fancy words that sound impressive (e.g. multi-colinearity, heteroskedasticity, etc)\n",
    "\n",
    "### Baseline Test\n",
    "\n",
    "##### Let's measure your current understanding of basic statistics\n",
    "\n",
    "It is the year 2395 and Mars has a problem. Their environment is failing and they need to relocate to another planet. The Martian president contacted the humans of Earth and negotiated a trial colonization for a small subset of their population. With much excitement, a spaceship carrying 40,000 Martians lands on Earth on a cool December morning. After a formal welcome reception, the visiting Martians went to get weighed. Their weights were measured and the computed mean of all of their weights was 200kg with a standard deviation of 20kg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statement 1\n",
    "\n",
    "The average Martian weighs 200kg"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statement 2\n",
    "\n",
    "95% of Martians weigh between 160.8kg and 239.2kg"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statement 3\n",
    "\n",
    "The weights of the visiting Martians can assumed to be normally distributed"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statement 4\n",
    "\n",
    "We can assume the standard deviation of Martians is 20kg"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Statistics?\n",
    "\n",
    "The dictionary definition of statistics is the practice or science of collecting and analyzing numerical data in large quantities, especially for the purpose of inferring proportions in a whole from those in a representative sample.\n",
    "\n",
    "We can break down statistics into two general categories:\n",
    "- Descriptive statistics\n",
    "- Inferential statistics\n",
    "\n",
    "Descriptive statistics utilizes numerical and graphical methods to look for patterns, to summarize information revealed, and to present the information in a convenient form. Exapmles of descriptive statistics might include:\n",
    "- A list of baseball players' batting averages\n",
    "- Pie chart with a breakdown of ethnic backgrounds of people in the U.S.\n",
    "- A histogram of household incomes in the U.S.\n",
    "\n",
    "Inferential statistics utilizes sample data to make estimates, decisions, predictions, or other generalizations about larger sets of data.\n",
    "- A pre-ballot poll asking who individuals would vote for to predict a winner\n",
    "- The average price of a basket of goods to measure CPI to measure inflation\n",
    "- Measuring the average weight of a bag of chips to determine whether machinery is performing as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random sampling\n",
    "\n",
    "The quintessential practice of statistical modeling is the idea of random sampling. All of inferential statistics is based on the idea of using random samples to make inferences about a larger group (often referred to as the \"population\").\n",
    "\n",
    "#### Why do we need to sample?\n",
    "\n",
    "In a nutshell, two reasons:\n",
    "1. It is often too costly and time consuming to measure statistics on the entire population\n",
    "2. The population statistics are not known or measurable\n",
    "\n",
    "##### Example 1\n",
    "\n",
    "Imagine we are trying to know whether or not a certain image or wording on a presidential candidate's webpage is more effective than another. Why would we want to run an experiment using random sampling? What kind of test would we set up?\n",
    "\n",
    "##### Example 2\n",
    "Imagine we are trying to determine ahead of an actual election whether one candidate is expected to win. Why would we want to sample this information? What kind of test would we set up?\n",
    "\n",
    "##### Example 3\n",
    "Imagine USF was doing an analysis of gender pay inequality. They want to know whether male professors make more than their female counterparts of similar tenure. Why would we want to sample this information? What kind of test would we set up?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Definitions\n",
    "\n",
    "Part of understanding how to do statistics is understanding what we are measuring. Here are a few key terms that we are going to use:\n",
    "- **Experimental unit**: an object (e.g. a person, thing, transaction, or event) on which we collect data\n",
    "- **Population**: the set of all experimental units that we are interested in understanding\n",
    "- **Variable**: the characteristic, property, or outcome of an individual experimental unit\n",
    "- **Sample**: a subset of the experimental units within a population\n",
    "- **Statistical inference**: an estimate or prediction or any other generalization about a population based on information contained in a sample.\n",
    "\n",
    "Let's practice our understanding of these key definitions and concepts given real-world situations. For each scenario below, let's identify and discuss the unit, population, possible variables, sampling methodology, and possible statistical inference that we would be working with.\n",
    "\n",
    "##### Scenario 1\n",
    "Determining the effectivness of an experimental drug for brain cancer treatment.\n",
    "\n",
    "##### Scenario 2\n",
    "Determining whether potato chip bag filler machine is working properly\n",
    "\n",
    "##### Scenario 3\n",
    "Determining what imagery and wording to use in a presidential candidate's webpage\n",
    "\n",
    "##### Scenario 4\n",
    "Determining whether prison inmates are being fed a proper diet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions\n",
    "\n",
    "Undersatnding distributions are a foundational element of statistics. There are hundreds of distributions in existance, most of which have highly specific use cases that you will never encounter. Today we will discuss the following:\n",
    "- Normal\n",
    "- Student's T\n",
    "- Uniform & Bernoulli\n",
    "- Binomial & Chi-squared\n",
    "- Poisson & negative exponential\n",
    "\n",
    "The following is an excellent overview of the statistics we are covering (plus a few more). You will need to review this for this week's homework and next week's quiz.\n",
    "https://blog.cloudera.com/blog/2015/12/common-probability-distributions-the-data-scientists-crib-sheet/\n",
    "\n",
    "Before we get into the details, I want to briefly go over the idea of probability density functions (pdf) and cumulative density functions (cdf).\n",
    "\n",
    "A probability density function (or density of a continuous random variable) is a function whose value can be interpreted as providing a relative likelihood that the value of the random variable would be equal to that value. However, since the likelihood of any random number being **exactly** equal to a value is infinitesimally small, we tend to deal with ranges. The area under a pdf curve between two points will give you the probability of a randomly selected number being in that range. Since the area under the curve is difficult to compute, we use the cumulative plot and take the difference in cdf values for the two points.\n",
    "\n",
    "<img src=\"http://work.thaslwanter.at/Stats/html/_images/PDF_CDF.png\" width=400 height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal Distribution\n",
    "The king of all distributions - also the most over-/mis-used of distributions - is the Normal Distribution. If I had a nickel for every time I heard someone say, \"if we assume the data to be normally distributed then....\" I'd be able to afford to teach full time! \n",
    "\n",
    "People love normal distributions because they are easy to understand. We are taught at a young age about \"bell curves\" and then at a later age about how large enough samples can allow us to assume normality of the data (we're not *actually* taught this, but rather we mishear it because we fail to understand the real lesson). Normal distributions are common in statistics but uncommon in real life. The reason they are common (and valid) in statistics is because we are typically talking about the properties of samples and statistical attributes such as mean values. In real life we tend to think about data in terms of individual observations (e.g. the distribution of weights in America).\n",
    "\n",
    "I want to start by showing you the origin of the fallcy regarding large sample sizes and normal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random_number_generators import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vals = 1000000\n",
    "pop1 = generator1(num_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_sample(nums, how_many):\n",
    "    sample = np.random.choice(nums, how_many)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop1_sample = get_random_sample(pop1, 10)\n",
    "pop1_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop1_sample.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this tell us about the mean of our first set of random numbers?\n",
    "\n",
    "What if we try this again?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop1_sample = get_random_sample(pop1, 10)\n",
    "pop1_sample.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a histogram of the data and see what we think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop1.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm.... a uniform distribution with values ranging from 0 to 20.\n",
    "\n",
    "Let's try to grab a bunch of samples, compute their means, then plot those means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lots_of_sample_means(nums, how_many_per_sample, how_many_samples):\n",
    "    sample_means = []\n",
    "    for i in range(how_many_samples):\n",
    "        sample = get_random_sample(nums, how_many_per_sample)\n",
    "        sample_mean = sample.mean()\n",
    "        sample_means.append(sample_mean)\n",
    "    return pd.Series(sample_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop1_sample_means = get_lots_of_sample_means(pop1, 10, 10)\n",
    "pop1_sample_means.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see this histogram build as we increase the number of samples included in our histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4,10))\n",
    "plt.title('Random Distributions')\n",
    "axes = []\n",
    "\n",
    "for i in range(5):\n",
    "    num_samples = (1 + 3**i) * 10\n",
    "    pop_sample_means = get_lots_of_sample_means(pop1, 10, num_samples)\n",
    "    ax = plt.subplot2grid((5,1), (i,0))\n",
    "    pop_sample_means.hist(ax=ax, density=True)\n",
    "    axes.append(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty normal to me! Let's do one more to make sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_sample_means = get_lots_of_sample_means(pop1, 10, 10000) #Generate histogram of 10,000 sample means\n",
    "pop_sample_means.hist(density=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you've just witnessed is the Central Limit Theorem in action. CLT states that the sampling distribution of the mean of any independent, random variable will be normal (or nearly normal), *if the sample size is large enough*... even if the original variables themselves are not normally distributed.\n",
    "\n",
    "In plain english, what CTL is saying is that the **means** of samples can be assumed to be normally distributed, assuming the samples are large enough. This says *nothing* of our ability to make inferences about the distribution of the underlying data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practical terms, what we are capable of saying is that given a large enough sample, I can estimate the confidence interval for where the true mean of a population is based on the mean and standard deviation of sufficient sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Student's T-Distribution to Understand Population Mean\n",
    "\n",
    "While I showed you how to generate the distribution of means based on many samples of data, we're typically only given a single sample. This sample is what we used to compute the confidence interval for the true population mean.\n",
    "\n",
    "Let's go back to taking a single measurement from pop1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 30\n",
    "sample = get_random_sample(pop1, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_mean = sample.mean()\n",
    "sample_sd = sample.std()\n",
    "print('mean:', sample_mean)\n",
    "print('std:', sample_sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula for determining the confidence interval for finding the true mean is:\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mu = \\bar{x} \\pm t\\frac{s}{\\sqrt{n}}\n",
    "\\end{equation*}\n",
    "where t can be found in the table below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www2.palomar.edu/users/rmorrissette/Lectures/Stats/ttests/TTable.jpg\" width=400 height=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confidence_interval(sample, sample_size):\n",
    "    t95 = 2.045\n",
    "    sample_mean = sample.mean()\n",
    "    sample_sd = sample.std()\n",
    "    confidence_interval = [sample_mean - t95 * sample_sd / np.sqrt(sample_size), sample_mean + t95 * sample_sd / np.sqrt(sample_size)]\n",
    "    return confidence_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_confidence_interval(sample, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does a 95% confidence interval *really* mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_interval_definition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confidence_interval_success(population):\n",
    "    true_mean_in_range = []\n",
    "    for i in range(50000):\n",
    "        sample = get_random_sample(population, 30)\n",
    "        ci = compute_confidence_interval(sample, 30)\n",
    "        if (ci[0] <= 10) and (ci[1] >= 10):\n",
    "            result = True\n",
    "        else:\n",
    "            result = False\n",
    "        true_mean_in_range.append(result)\n",
    "    pct_true = pd.Series(true_mean_in_range).mean()\n",
    "    return pct_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_confidence_interval_success(generator2(100000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Does the distribution of the underlying data matter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_to_analyze = generator5(num_vals)\n",
    "pop_to_analyze.hist(density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_sample_means = get_lots_of_sample_means(pop_to_analyze, 10, 10000)\n",
    "pop_sample_means.hist(density=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bernoulli & Uniform Distribution -> Binomial & Chi-Squared Distribution\n",
    "\n",
    "Bernoulli and Uniform Distributions are often used to measure the outcomes of trials. A Bernoulli trial would represent strictly two possible outcomes and uniform distributions being one of many outcomes (note that since these are discrete values, the pdf is now called a probability mass function, or pmf). Think of Bernoulli trial as a coin flip. Introductory examples typically use 50/50 probabilities for a heads or tails respectively, but it most definitely does not have to be. A Bernoulli trial could also represent whether someone clicked on an advertisement or not. It need only represent a situation with only two possible outcomes (e.g. click or no click).\n",
    "\n",
    "A uniform distribution can be thought of akin to rolling a die. There is a one-sixth probability of a roll landing on any single number. With uniform distributions, you are bound to equal distributions.\n",
    "\n",
    "Bernoulli and Uniform Distributions are not that useful by themselves. Rather they are inputs into more useful statistical tests utilizing the Binomial or Chi-Squared distributions (Chi-Squared actually has many use cases, we will only go over this one today).\n",
    "\n",
    "A binomial distribution represents the probability of a specific number of successes of a Bernoulli trial given a certain number of trials. Given the following definitions, we will then go over the formula.\n",
    "\n",
    "- x: The number of successes that result from the binomial experiment.\n",
    "- n: The number of trials in the binomial experiment.\n",
    "- P: The probability of success on an individual trial.\n",
    "\n",
    "\\begin{equation*}\n",
    "b(x|n,P) = \\frac{n!}{x!(n - x)!} * P^x * (1 - P_x)^{n - x}\n",
    "\\end{equation*}\n",
    "\n",
    "Thus, given 10 fair coin flips, the probability of getting exactly 5 heads is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import factorial\n",
    "n = 3\n",
    "x = 1\n",
    "P = .5\n",
    "(factorial(n) / (factorial(x)*factorial(n-x))) * P**x * (1 - P)**(n-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The possible outcomes are:\n",
    "- HHH\n",
    "- HHT\n",
    "- HTH\n",
    "- THH\n",
    "- HTT\n",
    "- THT\n",
    "- TTH\n",
    "- TTT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted the probabilities of all possible number of heads given n trials, we could write the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_single_binomial_probability(num_trials, num_successes, prob):\n",
    "    n = num_trials\n",
    "    x = num_successes\n",
    "    p = prob\n",
    "    \n",
    "    p_x_successes = (factorial(n) / (factorial(x)*factorial(n-x))) * (p**x) * ((1 - p)**(n-x))\n",
    "    return p_x_successes\n",
    "\n",
    "\n",
    "def generate_all_probabilities(num_trials, prob):\n",
    "    all_probs = []\n",
    "    for i in range(num_trials + 1):\n",
    "        binomial_prob = compute_single_binomial_probability(num_trials, i, prob)\n",
    "        all_probs.append(binomial_prob)\n",
    "    return pd.Series(all_probs)\n",
    "\n",
    "coin_flip_10 = generate_all_probabilities(10, .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_single_binomial_probability(10, 5, .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coin_flip_10.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There's a library for this too :)\n",
    "import scipy.stats as ss\n",
    "coin_flip_10 = ss.binom(10, .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coin_flip_10.pmf(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chi-squared test is used to determine whether there is a statistically significant difference between the expected frequencies of possible outcomes and the observed frequencies. Similarly to the binomial test and testing a coin, the chi-squared test can be used to measure the fairness of a die. We will go over the specifics of this test next week as we go more in-depth regarding hypothesis testing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
